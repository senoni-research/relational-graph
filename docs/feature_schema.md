# VN2 Graph Feature Schema

## Overview

Graph model features for VN2 inventory planning, generated by `scripts/orders_vn2.py`. These features provide calibrated activation probabilities and zero-inflated demand moments for use in:
- HB model covariates (improve forecast accuracy)
- Post-processing / blending (optimize orders via gating/capping)
- Cost-based decision making (choose per-row between HB and graph)

---

## Temporal Guarantees

**Strict pre-`as_of` guarantee**: All predictions use only data strictly before the `as_of` date. The loader enforces `edge.time < as_of` and will fail hard if violated.

**Recommended Boundaries**:
- For Week 0 (2024-04-08) submission:
  - `as_of`: 2024-04-08 (Week 0 Monday)
  - `train_end`: 2024-01-31 (to leave Feb/Mar for downstream CV)
  - `val_end`: 2024-03-15 (calibration window)
  - `start_week`: 2024-04-08 (horizon starts here)

**Calibration Isolation**:
- Calibrator fits on validation window (`train_end` < t ≤ `val_end`)
- Downstream HB CV should use **non-overlapping** folds (e.g., Feb 05–Feb 26) to avoid double-dipping
- Check `calibrator.meta.json` for actual calibration window and ECE/Brier metrics

---

## File Outputs

### 1. Full Features (`orders_features_*.csv`)
- **Rows**: All store×product pairs (typically 19,899)
- **Use**: Exploration, full-universe HB covariates

### 2. 599-Row Features (`orders_features_599_*.csv`)
- **Rows**: 599 pairs aligned to submission template index
- **Use**: Direct merge into HB submission for blending or covariate join

### 3. Metadata Sidecar (`*.meta.json`)
- Accompanies each artifact with:
  - `as_of`: Temporal boundary
  - `train_end`, `val_end`, `horizon`, `hops`, `K`: Training/eval configuration
  - `counts`: Edge/node/pair counts
  - `code_version`: Git SHA for reproducibility
  - `ece`, `brier`: Calibration reliability (if calibrator used)

---

## Column Definitions

| Column | Type | Definition | Unit | Use |
|--------|------|------------|------|-----|
| `store_id` | str | Store identifier | — | Join key |
| `product_id` | str | Product identifier | — | Join key |
| `start_week` | int | First week of horizon (YYYYMMDD) | YYYYMMDD | Temporal anchor |
| `horizon_weeks` | int | Forecast horizon length | weeks | Typically 3 |
| **Activation Probabilities** |
| `p_t1` | float | P(sold > 0) for week 1 | [0,1] | HB covariate, gating |
| `p_t2` | float | P(sold > 0) for week 2 | [0,1] | HB covariate |
| `p_t3` | float | P(sold > 0) for week 3 | [0,1] | **Primary gating signal** |
| `p_mean` | float | Average activation prob across horizon | [0,1] | Summary |
| `p_sum` | float | Sum of activation probs (≤ horizon) | [0, H] | Derived |
| **Conditional Demand (Given Active)** |
| `mu_hat_plus` | float | Mean units sold per week \| sold > 0 | units/week | Per-product baseline |
| `sigma_hat_plus` | float | Std of units sold per week \| sold > 0 | units/week | Per-product baseline |
| **Weekly Zero-Inflated Moments** |
| `mu_w1` | float | Expected demand week 1 (p_t1 × mu_hat_plus) | units | HB week-specific covariate |
| `mu_w2` | float | Expected demand week 2 (p_t2 × mu_hat_plus) | units | HB week-specific covariate |
| `mu_w3` | float | Expected demand week 3 (p_t3 × mu_hat_plus) | units | HB week-specific covariate |
| `sigma_w1` | float | Std of demand week 1 (zero-inflated) | units | HB week-specific covariate |
| `sigma_w2` | float | Std of demand week 2 (zero-inflated) | units | HB week-specific covariate |
| `sigma_w3` | float | Std of demand week 3 (zero-inflated) | units | HB week-specific covariate |
| **Horizon Aggregates** |
| `mu_H` | float | Expected total demand over horizon (Σ mu_wk) | units | Base-stock μ parameter |
| `sigma_H` | float | Std of total demand over horizon (√Σ var_wk) | units | Base-stock σ parameter |

---

## Usage Examples

### 1. Merge into HB as Covariates

```python
import pandas as pd

# Load HB training data
hb_train = pd.read_csv("hb_training_data.csv")

# Load graph features (599-row or full)
feat = pd.read_csv("artifacts/orders_features_599_idemb_gated2.csv")

# Normalize IDs
for df in [hb_train, feat]:
    df["store_id"] = df["store_id"].astype(str)
    df["product_id"] = df["product_id"].astype(str)

# Merge
hb_train = hb_train.merge(
    feat[["store_id", "product_id", "p_t1", "p_t2", "p_t3", 
          "mu_w1", "mu_w2", "mu_w3", "sigma_w1", "sigma_w2", "sigma_w3"]],
    on=["store_id", "product_id"],
    how="left"
)

# Use in HB formula (example: add p_t3 as activation covariate)
# Your HB model can now use these as additional predictors
```

### 2. Post-Process: Choose Lower-Cost Order Per Row

```python
import numpy as np
import math

# Expected cost helper
SQRT2PI = math.sqrt(2.0 * math.pi)

def expected_cost(q, mu_H, sigma_H, onhand, onorder, C_u=1.0, C_h=0.2, H=3):
    S = onhand + onorder + q
    sigma_H = max(1e-6, sigma_H)
    z = (S - mu_H) / sigma_H
    phi = np.exp(-0.5 * z * z) / SQRT2PI
    Phi = 0.5 * (1.0 + math.erf(z / math.sqrt(2.0)))
    shortage = sigma_H * phi + (mu_H - S) * (1.0 - Phi)
    overage = sigma_H * phi + (S - mu_H) * Phi
    return C_u * max(0, shortage) + (C_h * H) * max(0, overage)

# Load features, HB, and state
feat = pd.read_csv("artifacts/orders_features_599_*.csv")
hb = pd.read_csv("hb_submission.csv")
state = pd.read_csv("initial_state.csv")

# ... merge and normalize IDs ...

# Compute graph order (base-stock)
from scipy.stats import norm
beta = 0.833
z = norm.ppf(beta)
df["q_graph"] = np.maximum(0, 
    df["mu_H"] + z * df["sigma_H"] - df["onhand"] - df["onorder_le2"]
).round().astype(int)

# Compute costs
df["cost_hb"] = df.apply(lambda r: expected_cost(
    r["hb_qty"], r["mu_H"], r["sigma_H"], r["onhand"], r["onorder_le2"]
), axis=1)

df["cost_graph"] = df.apply(lambda r: expected_cost(
    r["q_graph"], r["mu_H"], r["sigma_H"], r["onhand"], r["onorder_le2"]
), axis=1)

# Choose lower cost per row
df["order_qty"] = np.where(df["cost_graph"] < df["cost_hb"], df["q_graph"], df["hb_qty"])

# Write submission
df[["store_id", "product_id", "order_qty"]].to_csv("submission_best.csv", index=False)
```

### 3. Two-Sided Gating (ABC-aware)

Use `scripts/orders_vn2.py` with `--blend gated2`:
```bash
python -u scripts/orders_vn2.py \
  --graph artifacts/vn2_graph_full_temporal_v2.jsonl \
  --ckpt artifacts/checkpoints/v2_scorer_idemb.pt \
  --calibrator artifacts/checkpoints/calibrators/iso_val_2024-03-15_idemb.pkl \
  --as-of 2024-04-08 \
  --train-end 2024-01-31 --start-week 20240408 --horizon 3 \
  --hops 1 --K 30 \
  --out artifacts/orders_features.csv \
  --submission-index artifacts/sales_index.csv \
  --features-599 artifacts/orders_features_599.csv \
  --hb hb_submission.csv \
  --state "initial_state.csv" \
  --blend gated2 --grid --tau-grid 0.55,0.60,0.65 \
  --abc-quantiles 0.6,0.9 --beta-a 0.88 --beta-b 0.80 --beta-c 0.70 \
  --simulate-cost --cost-shortage 1.0 --cost-holding 0.2 \
  --submit-blended artifacts/orders_blended.csv
```

---

## Feature Computation

### Activation Probability (`p_tk`)
- Calibrated probability that `(store, product, week_k)` will have `sold > 0`
- Computed via GNN edge scorer + isotonic/Platt calibration
- Trained on inventory-aware negatives (sold=0 ∧ has_inventory=True)

### Weekly Moments (`mu_wk`, `sigma_wk`)
Zero-inflated per-week distribution:
```
D_k ~ (1 - p_tk) × δ₀  +  p_tk × N(mu_hat_plus, sigma_hat_plus²)
E[D_k] = p_tk × mu_hat_plus
Var[D_k] = p_tk × (sigma_hat_plus² + (1-p_tk) × mu_hat_plus²)
```

### Horizon Aggregates (`mu_H`, `sigma_H`)
Assuming independent weeks (conservative):
```
D_H = Σ_{k=1..H} D_k
mu_H = Σ mu_wk
sigma_H = √(Σ var_wk)
```

**Note**: Weekly moments (`mu_wk`, `sigma_wk`) avoid the i.i.d. assumption and are preferred for HB covariates.

---

## Validation Checklist

Before using features in production:
1. ✅ Check `*.meta.json` for `as_of` and `train_end` alignment
2. ✅ Verify `calibrator.meta.json` ECE < 0.05, Brier < 0.20
3. ✅ Confirm calibration window doesn't overlap with your CV folds
4. ✅ Inspect reliability diagram (`*_reliability.png`) for bin alignment
5. ✅ Ensure feature file has 599 rows in correct index order
6. ✅ Validate no negative `mu_*` or `sigma_*` values

---

## Known Limitations

- **No store-level heterogeneity**: Currently treats all stores as exchangeable (except via graph structure)
- **Fixed horizon**: Only generates 3-week features; extend `--horizon` for longer periods
- **I.I.D. week assumption**: `sigma_H` assumes independent weekly variance; use weekly moments for correlated demand

---

## Contact & Support

For questions or issues:
- Check `docs/VN2_TRAINING_SUMMARY.md` for training pipeline
- Check `docs/onboarding.md` for RELIA architecture details
- Review `.meta.json` sidecars for artifact provenance

